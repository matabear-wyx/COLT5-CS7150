<!doctype html>
<html lang="en">
<head>
<title>COLT5 for CS7150</title>
<meta property="og:title" content="COLT5 for CS7150" />
<meta name="twitter:title" content="COLT5 for CS7150" />
<meta name="description" content="CS 7150 Report for COLT5: Faster Long-Range Transformers with Conditional Computation" />
<meta property="og:description" content="CS 7150 Report for COLT5: Faster Long-Range Transformers with Conditional Computation" />
<meta name="twitter:description" content="CS 7150 Report for COLT5: Faster Long-Range Transformers with Conditional Computation" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">COLT5</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of COLT5: Faster Long-Range Transformers with Conditional Computation</h2>
    <p>Optimizing Transformer models for long context windows has always been a problem. In the original transformer decoder implementation, to generate a sequence of length N, the time and memory complexity is respectively O(n<sup>2</sup>) and O(n). In this work, the authors proposed a way to optimize the coefficient of the n<sup>2</sup> term, from 2 to 1/84.</p>
    <img src="./imgs/figure1.png" alt="Figure 1" >
    <p>The optimization is done with a "routing" layer: by adding a "light" attention and mlp alongside the original "heavy" attention and mlp, the model can use more computation power on the important tokens, and use less computational power on unimportant tokens. The authors trained a model with a 64k context window, yielding higher performance (F1 score) in less inference time than LongT5 on the NarrativeQA dataset.</p>
    <img src="./imgs/figure2.png" alt="Figure 2" >
    <p>This method also improves the interpretability of the Transformer model. The routing score can be used as an importance score of the token.</p>
</div>
</div>
<div class="row">
<div class="col">

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>Literature Review</h2>
    <p>Efficient Streaming Language Models with Attention Sinks(2023):
    </p>
    <p>Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup.
    </p>
    <p>Think before you speak: Training Language Models With Pause Tokens(2023):
    </p>
    <p>Language models generate responses by producing a series of tokens in immediate succession: the (K+1)th token is an outcome of manipulating K hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, K+10 hidden vectors, before it outputs the (K+1)th token? We operationalize this idea by performing training and inference on language models with a (learnable) pause token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate pause-training on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of 18% EM score on the QA task of SQuAD, 8% on CommonSenseQA and 1% accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.
    </p>
    <p>Longformer: The Long-Document Transformer(2020):
    </p>
    <p>Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.
    </p>
</div>
</div>
<div class="row">
<div class="col">

<div class="container">
  <h2>Biography</h2>
  <div class="row">
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/Joshua.png" alt="Joshua Ainslie">
        <div class="card-body">
          <h5 class="card-title">Joshua Ainslie</h5>
          <p class="card-text">MS in Statistic at Stanford; Software Engineer at Google</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/Tao.jpg" alt="Tao Lei">
        <div class="card-body">
          <h5 class="card-title">Tao Lei</h5>
          <p class="card-text">PhD at MIT, Research scientist at Google Brain</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/Michiel.jpg" alt="Michiel de Jong">
        <div class="card-body">
          <h5 class="card-title">Michiel de Jong</h5>
          <p class="card-text">PhD at USC; Research Scientist at Stealth Startup; Former Googler</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/Santiago.jpg" alt="Santiago Ontanon">
        <div class="card-body">
          <h5 class="card-title">Santiago Ontanon</h5>
          <p class="card-text">PhD at Autonomous University of Barcelona; Associate Professor at Drexel University</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/Siddhartha.jpg" alt="Siddhartha Brahma">
        <div class="card-body">
          <h5 class="card-title">Siddhartha Brahma</h5>
          <p class="card-text">PhD at EPFL; Research Scientist at Google Deepmind</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/Yury.png" alt="Yury Zemlyanskiy">
        <div class="card-body">
          <h5 class="card-title">Yury Zemlyanskiy</h5>
          <p class="card-text">PhD student at USC</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/David.jpg" alt="David Uthus">
        <div class="card-body">
          <h5 class="card-title">David Uthus</h5>
          <p class="card-text">PhD at The University of Auckland, Software Engineer at Google</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/Guo.jpg" alt="Mandy Guo">
        <div class="card-body">
          <h5 class="card-title">Mandy Guo</h5>
          <p class="card-text">BS at Cornell, Software Engineer at Google</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/James.jpg" alt="James Lee-Thorp">
        <div class="card-body">
          <h5 class="card-title">James Lee-Thorp</h5>
          <p class="card-text">Google Researcher</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/Yi.jpg" alt="Yi Tay">
        <div class="card-body">
          <h5 class="card-title">Yi Tay</h5>
          <p class="card-text">Senior Research Scientist at Google Brain</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/Sung.jpg" alt="Yun-Hsuan Sung">
        <div class="card-body">
          <h5 class="card-title">Yun-Hsuan Sung</h5>
          <p class="card-text">PhD at Stanford; Senior Research Scientist at Google</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 col-lg-4">
      <div class="card">
        <img class="card-img-top" src="./imgs/Sumit.jpg" alt="Sumit Sanghai">
        <div class="card-body">
          <h5 class="card-title">Sumit Sanghai</h5>
          <p class="card-text">Software Engineer at Google</p>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <h2>Academic Impact</h2>
  <p>This work enables a new research question: optimizing the transformer at a token level. This work assumes that each token has a different importance in the sequence, so they can allocate less computational power to the less important token, thus reducing overall time complexity. Another interesting research question is, can we add more computation power to a specific token? (Think before you speak: Training Language Models With Pause Tokens) studied this method, and reported a significant performance gain when allowing more computation of the important output tokens.
  </p>
</div>

<div class="container">
  <h2>Industry Impact</h2>
  <p>The cost of running LLMs is a huge burden to be deployed everywhere. For example, the "new bing" chatbot has multiple versions of model sizes, and they have a small "router" in front of all incoming requests to tell the "difficulty" of the question. However, the performance of this router is very bad, so you'll often see new bing replies silly responses for tricky questions.
  </p>
  <p>So, optimizing LLMs for longer contexts and fewer computations can help the industry a lot.
  </p>
</div>

<div class="container">
  <h2>Review from Yuxiong Wu</h2>
  <p>Score: 8/10 (Strong Accept)
  </p>
  <p>Pros:
  </p>
  <p>Innovation: Implements a conditional computation mechanism for efficient processing of long texts.
  </p>
  <p>Efficiency: Significantly improves training and inference speed, especially with extremely long inputs.
  </p>
  <p>Scalability: Effectively handles long inputs up to 64k tokens.
  </p>
  <p>Few-Shot Learning Capabilities: Performs well in few-shot learning tasks.
  </p>
  <p>Cons:
  </p>
  <p>Limited to Encoder: Conditional computation is only applied to the encoder, not suitable for token-by-token generation in decoders.
  </p>
  <p>Specialization & Adaptability: Primarily designed for long sequences, might not be suitable for short sequences or require training from scratch.
  </p>
  <p>Decoder Integration: No exploration of how to integrate this model with decoders, limiting its application in decoder-only models.
  </p>
</div>

<div class="container">
  <h2>Review from Yuxuan Lu</h2>
  <p>Score: 8/10 (Strong Accept)
  </p>
  <p>Pros:
  </p>
  <p>Novel idea: Optimizing transformers with Comditional Computation is really interesting and novel idea
  </p>
  <p>Results: Outperforms other methods while takes less computational power
  </p>
  <p>Cons:
  </p>
  <p>Can't be applied to transformer decoders, which most recent model is based on
  </p>
  <p>Require training from scratch, can't "plug-and-play"
  </p>
</div>

<h3>References</h3>

<p><a name="Training Language Models With Pause Tokens">[1]</a> <a href="https://arxiv.org/pdf/2310.02226.pdf"
  >Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, Vaishnavh Nagarajan.
  <em>Think before you speak: Training Language Models With Pause Tokens.</em></a>
  ICLR 2024
</p>

<p><a name="Efficient Streaming Language Models with Attention Sinks">[1]</a> <a href="https://arxiv.org/pdf/2309.17453.pdf"
  >Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis.
  <em>Efficient Streaming Language Models with Attention Sinks.</em></a>
  ICLR 2024
</p>

<p><a name="Longformer: The Long-Document Transformer">[1]</a> <a href="https://arxiv.org/pdf/2004.05150.pdf"
  >Iz Beltagy, Matthew E. Peters, Arman Cohan.
  <em>Longformer: The Long-Document Transformer.</em></a>
  2020
</p>

<h2>Team Members</h2>
                                                   
<p>Yuxiong WU & Yuxuan LU</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
